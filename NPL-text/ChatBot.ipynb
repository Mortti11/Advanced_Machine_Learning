{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "18872834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import pickle\n",
    "import re\n",
    "from tensorflow.keras import layers , activations , models , preprocessing, utils\n",
    "\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, TimeDistributed\n",
    "from tensorflow.keras.models  import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "84bd75a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "with open('Intent.json') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "74829246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop through each intents dictionary \n",
    "# lists of sentences and responses\n",
    "questions  = []\n",
    "responses = []\n",
    "\n",
    "for intent in data[\"intents\"]:\n",
    "    for q in intent[\"text\"]:\n",
    "        questions.append(q)\n",
    "        responses.append(intent[\"responses\"][0])\n",
    "# I am using the first response for each intent\n",
    "# There is another way to pick a random response from the list of responses\n",
    "# because in responses there are multiple responses for each intent \n",
    "# I got the main structure code from lecture notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "c218ed66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n",
      "143\n"
     ]
    }
   ],
   "source": [
    "# Let's check the length of questions and responses\n",
    "print(len(questions))      \n",
    "print(len(responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "e635aa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to clean the question and response list \n",
    "def clean(s):\n",
    "    return re.sub(r\"[^a-z0-9\\s]\",\"\", s.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "1cf1392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the clean function to the questions and responses\n",
    "questions = [clean(q) for q in questions]\n",
    "answers   = [clean(r) for r in responses]\n",
    "\n",
    "# I will  add <start> and <end> tokens to the decoder input/output in seq2seq models\n",
    "# During training, the decoder learns to predict the next token based on the previous tokens\n",
    "answers   = [f'<START> {answer} <END>' for answer in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "d29b3afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 169\n"
     ]
    }
   ],
   "source": [
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(questions + answers)\n",
    "VOCAB_SIZE = len(tokenizer.word_index)+1\n",
    "print('vocab size:', VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "f2fc9b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer For later use during inference\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "b385840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use the fitted tokenizer to convert the cleaned questions and answers\n",
    "# (with start/end tokens) into sequences of integers.\n",
    "questions_seq = tokenizer.texts_to_sequences(questions)\n",
    "answers_seq = tokenizer.texts_to_sequences(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "08c2d145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max question length: 9\n",
      "Max answer length: 21\n"
     ]
    }
   ],
   "source": [
    "# Padding Sequences:\n",
    "# I will pad the sequences to ensure that all sequences have the same length.\n",
    "max_len_questions = max(len(seq) for seq in questions_seq)\n",
    "max_len_answers = max(len(seq) for seq in answers_seq)\n",
    "\n",
    "print(f'Max question length: {max_len_questions}')\n",
    "print(f'Max answer length: {max_len_answers}')\n",
    "\n",
    "padded_questions = pad_sequences(questions_seq, maxlen=max_len_questions, padding='post')\n",
    "padded_answers = pad_sequences(answers_seq, maxlen=max_len_answers, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "391c76c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.array(padded_questions)\n",
    "decoder_input_data = np.array(padded_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "f5d1b500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143, 21, 169)\n"
     ]
    }
   ],
   "source": [
    "shifted = [seq[1:] for seq in answers_seq]\n",
    "padded_answers = pad_sequences(\n",
    "    shifted,\n",
    "    maxlen = max_len_answers,\n",
    "    padding = \"post\"\n",
    ")\n",
    "decoder_target_data = to_categorical(padded_answers, num_classes=VOCAB_SIZE)\n",
    "print(decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "5bba7c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143, 21, 169)\n",
      "(143, 9)\n",
      "(143, 21)\n"
     ]
    }
   ],
   "source": [
    "# Turn them into NumPy arrays for Keras\n",
    "print(decoder_target_data.shape)    \n",
    "print(encoder_input_data.shape)  \n",
    "print(decoder_input_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7f6581",
   "metadata": {},
   "source": [
    "**Building the LSTM Chatbot Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "7db74bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "LATENT_DIM = 128    \n",
    "BATCH_SIZE = 64      \n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "04ff05b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Encoder inputs: a batch of padded question token IDs, shape = (batch, maxlen_questions)\n",
    "encoder_inputs = Input(shape=(max_len_questions,), name=\"encoder_inputs\")\n",
    "\n",
    "# 2) Embed those IDs into dense vectors\n",
    "enc_emb = Embedding(VOCAB_SIZE, EMBEDDING_DIM, mask_zero=True, name=\"encoder_embedding\")(encoder_inputs)\n",
    "\n",
    "# 3) Run an LSTM over the embeddings and grab only its final states\n",
    "#    LATENT_DIM is the size of the hidden/cell vectors\n",
    "_, state_h, state_c = LSTM(LATENT_DIM, return_state=True, name=\"encoder_lstm\")(enc_emb)\n",
    "\n",
    "# 4) Bundle the final LSTM states to pass to the decoder\n",
    "encoder_states = [state_h, state_c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "e08a6be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Decoder inputs: padded answer token IDs (includes your <start> token up front)\n",
    "decoder_inputs = Input(shape=(max_len_answers,), name=\"decoder_inputs\")\n",
    "\n",
    "# 2) Embed those IDs into vectors\n",
    "dec_emb = Embedding(VOCAB_SIZE, EMBEDDING_DIM, mask_zero=True, name=\"decoder_embedding\")(decoder_inputs)\n",
    "\n",
    "# 3) Run an LSTM over the embeddings, seeding it with the encoder’s final states\n",
    "#    return_sequences=True so we get an output at each time step\n",
    "dec_lstm, _, _ = LSTM(\n",
    "    LATENT_DIM,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    name=\"decoder_lstm\"\n",
    ")(dec_emb, initial_state=encoder_states)\n",
    "\n",
    "# 4) Turn each LSTM output into a softmax over the vocab\n",
    "decoder_outputs = Dense(VOCAB_SIZE, activation=\"softmax\", name=\"decoder_dense\")(dec_lstm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "cfe6addc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_27\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_27\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,900</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_65        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,900</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │ encoder_embeddin… │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),      │            │ not_equal_65[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), │    <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │ decoder_embeddin… │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),      │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]      │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_dense       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">169</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">21,801</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m100\u001b[0m)    │     \u001b[38;5;34m16,900\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_65        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m100\u001b[0m)   │     \u001b[38;5;34m16,900\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),     │    \u001b[38;5;34m117,248\u001b[0m │ encoder_embeddin… │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),      │            │ not_equal_65[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m128\u001b[0m), │    \u001b[38;5;34m117,248\u001b[0m │ decoder_embeddin… │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),      │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]      │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_dense       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m169\u001b[0m)   │     \u001b[38;5;34m21,801\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">290,097</span> (1.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m290,097\u001b[0m (1.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">290,097</span> (1.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m290,097\u001b[0m (1.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Print a summary of the model architecture and parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "57f01102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "fe2e0a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 395ms/step - accuracy: 0.0403 - loss: 5.1247 - val_accuracy: 0.2447 - val_loss: 5.1177\n",
      "Epoch 2/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.3346 - loss: 5.0960 - val_accuracy: 0.5140 - val_loss: 5.1065\n",
      "Epoch 3/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.7665 - loss: 5.0683 - val_accuracy: 0.5140 - val_loss: 5.0918\n",
      "Epoch 4/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8079 - loss: 5.0303 - val_accuracy: 0.4877 - val_loss: 5.0688\n",
      "Epoch 5/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.8278 - loss: 4.9717 - val_accuracy: 0.4877 - val_loss: 5.0231\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "history = model.fit([encoder_input_data, decoder_input_data], # Input data (list)\n",
    "                    decoder_target_data,                     # Target data\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_split=0.2)                    # Use 20% of data for validation\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de65964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_answers = utils.to_categorical(padded_answers , VOCAB_SIZE)\n",
    "decoder_output_data = np.array(onehot_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5356d019",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_id = len(tokenizer.word_index) + 1\n",
    "end_id   = start_id + 1\n",
    "VOCAB = end_id + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "b6603972",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_seqs = [[start_id] + seq + [end_id] for seq in answer_seqs]\n",
    "question_seqs = tokenizer.texts_to_sequences(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "65e8fd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_q = max(len(s) for s in question_seqs)\n",
    "max_a = max(len(s) for s in answer_seqs)\n",
    "enc_in = pad_sequences(question_seqs, maxlen=max_q, padding=\"post\")\n",
    "dec_in = pad_sequences(answer_seqs,   maxlen=max_a, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9384a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.array(padded_questions)\n",
    "decoder_input_data = np.array(padded_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "7db9f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tar = np.zeros_like(dec_in)\n",
    "for i, seq in enumerate(dec_in):\n",
    "    dec_tar[i, :-1] = seq[1:]\n",
    "dec_tar = to_categorical(dec_tar, num_classes=VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "55643650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, TimeDistributed\n",
    "from tensorflow.keras.models  import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "cc29165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Encoder\n",
    "enc_inputs = Input(shape=(None,), name=\"encoder_inputs\")\n",
    "# turn word-IDs → 64-d vectors\n",
    "enc_emb    = Embedding(input_dim=VOCAB, output_dim=100, mask_zero=True)(enc_inputs)\n",
    "# run through an LSTM, keep its final h/c\n",
    "_, state_h, state_c = LSTM(128, return_state=True, name=\"encoder_lstm\")(enc_emb)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# 2) Decoder\n",
    "dec_inputs = Input(shape=(None,), name=\"decoder_inputs\")\n",
    "dec_emb    = Embedding(input_dim=VOCAB, output_dim=100, mask_zero=True)(dec_inputs)\n",
    "# LSTM returns a full sequence plus new h/c, seeded by the encoder’s states\n",
    "dec_lstm, _, _ = LSTM(\n",
    "    128,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    name=\"decoder_lstm\"\n",
    ")(dec_emb, initial_state=encoder_states)\n",
    "# project each time-step to a softmax over the vocab\n",
    "dec_outputs = Dense(VOCAB, activation=\"softmax\", name=\"decoder_dense\")(dec_lstm)\n",
    "\n",
    "# 3) Build & compile\n",
    "model = Model([enc_inputs, dec_inputs], dec_outputs)\n",
    "model.compile(\n",
    "    optimizer=\"adam\",   \n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "00b2480e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 464ms/step - accuracy: 0.0357 - loss: 5.1320 - val_accuracy: 0.1136 - val_loss: 5.1219\n",
      "Epoch 2/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.1260 - loss: 5.1058 - val_accuracy: 0.1111 - val_loss: 5.1029\n",
      "Epoch 3/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.1155 - loss: 5.0712 - val_accuracy: 0.1111 - val_loss: 5.0716\n",
      "Epoch 4/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.1111 - loss: 5.0168 - val_accuracy: 0.1111 - val_loss: 5.0116\n",
      "Epoch 5/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.1111 - loss: 4.9223 - val_accuracy: 0.1111 - val_loss: 4.8768\n",
      "Epoch 6/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.1111 - loss: 4.7336 - val_accuracy: 0.1111 - val_loss: 4.5965\n",
      "Epoch 7/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.1111 - loss: 4.4099 - val_accuracy: 0.1111 - val_loss: 4.3840\n",
      "Epoch 8/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.1096 - loss: 4.0946 - val_accuracy: 0.1111 - val_loss: 4.3764\n",
      "Epoch 9/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.1084 - loss: 3.8586 - val_accuracy: 0.1111 - val_loss: 4.3594\n",
      "Epoch 10/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.1088 - loss: 3.6853 - val_accuracy: 0.1111 - val_loss: 4.3314\n",
      "Epoch 11/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.1119 - loss: 3.5434 - val_accuracy: 0.1284 - val_loss: 4.3293\n",
      "Epoch 12/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.3079 - loss: 3.4404 - val_accuracy: 0.5309 - val_loss: 4.3394\n",
      "Epoch 13/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.7130 - loss: 3.3245 - val_accuracy: 0.5309 - val_loss: 4.3438\n",
      "Epoch 14/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.7269 - loss: 3.2115 - val_accuracy: 0.5605 - val_loss: 4.3427\n",
      "Epoch 15/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.4759 - loss: 3.1235 - val_accuracy: 0.1309 - val_loss: 4.3390\n",
      "Epoch 16/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.1235 - loss: 3.0942 - val_accuracy: 0.1309 - val_loss: 4.3359\n",
      "Epoch 17/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.1188 - loss: 3.0395 - val_accuracy: 0.1309 - val_loss: 4.3599\n",
      "Epoch 18/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.1167 - loss: 2.9671 - val_accuracy: 0.1309 - val_loss: 4.4053\n",
      "Epoch 19/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.1173 - loss: 2.9447 - val_accuracy: 0.1827 - val_loss: 4.4365\n",
      "Epoch 20/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.1360 - loss: 2.9017 - val_accuracy: 0.2198 - val_loss: 4.4556\n",
      "Epoch 21/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.1819 - loss: 2.8355 - val_accuracy: 0.5605 - val_loss: 4.4851\n",
      "Epoch 22/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.5257 - loss: 2.7765 - val_accuracy: 0.5679 - val_loss: 4.5227\n",
      "Epoch 23/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.6117 - loss: 2.7504 - val_accuracy: 0.5605 - val_loss: 4.5365\n",
      "Epoch 24/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.4716 - loss: 2.6930 - val_accuracy: 0.5185 - val_loss: 4.5429\n",
      "Epoch 25/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.4053 - loss: 2.6271 - val_accuracy: 0.5852 - val_loss: 4.5690\n",
      "Epoch 26/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.6366 - loss: 2.6116 - val_accuracy: 0.5679 - val_loss: 4.5902\n",
      "Epoch 27/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.7506 - loss: 2.5346 - val_accuracy: 0.5679 - val_loss: 4.5962\n",
      "Epoch 28/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.7506 - loss: 2.5197 - val_accuracy: 0.5679 - val_loss: 4.6211\n",
      "Epoch 29/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.7463 - loss: 2.4748 - val_accuracy: 0.5679 - val_loss: 4.6592\n",
      "Epoch 30/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.7488 - loss: 2.4267 - val_accuracy: 0.5679 - val_loss: 4.6643\n",
      "Epoch 31/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.7554 - loss: 2.3777 - val_accuracy: 0.5679 - val_loss: 4.6764\n",
      "Epoch 32/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.7508 - loss: 2.3694 - val_accuracy: 0.5679 - val_loss: 4.7003\n",
      "Epoch 33/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.7558 - loss: 2.3074 - val_accuracy: 0.5679 - val_loss: 4.7007\n",
      "Epoch 34/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.7712 - loss: 2.2408 - val_accuracy: 0.5679 - val_loss: 4.7296\n",
      "Epoch 35/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.7757 - loss: 2.2564 - val_accuracy: 0.5679 - val_loss: 4.7472\n",
      "Epoch 36/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.7932 - loss: 2.1595 - val_accuracy: 0.5852 - val_loss: 4.7520\n",
      "Epoch 37/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.7757 - loss: 2.1709 - val_accuracy: 0.5679 - val_loss: 4.7820\n",
      "Epoch 38/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.7934 - loss: 2.0812 - val_accuracy: 0.5852 - val_loss: 4.7797\n",
      "Epoch 39/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.7789 - loss: 2.1052 - val_accuracy: 0.5852 - val_loss: 4.7957\n",
      "Epoch 40/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.7791 - loss: 2.0653 - val_accuracy: 0.5852 - val_loss: 4.8122\n",
      "Epoch 41/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.7874 - loss: 2.0182 - val_accuracy: 0.5852 - val_loss: 4.8174\n",
      "Epoch 42/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.7874 - loss: 1.9976 - val_accuracy: 0.5852 - val_loss: 4.8300\n",
      "Epoch 43/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8002 - loss: 1.9269 - val_accuracy: 0.5852 - val_loss: 4.8381\n",
      "Epoch 44/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.7876 - loss: 1.9441 - val_accuracy: 0.5679 - val_loss: 4.8448\n",
      "Epoch 45/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8065 - loss: 1.8766 - val_accuracy: 0.5852 - val_loss: 4.8595\n",
      "Epoch 46/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8127 - loss: 1.8155 - val_accuracy: 0.5852 - val_loss: 4.8787\n",
      "Epoch 47/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8113 - loss: 1.8123 - val_accuracy: 0.5852 - val_loss: 4.8880\n",
      "Epoch 48/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8111 - loss: 1.7924 - val_accuracy: 0.5679 - val_loss: 4.9138\n",
      "Epoch 49/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8108 - loss: 1.7610 - val_accuracy: 0.5852 - val_loss: 4.9208\n",
      "Epoch 50/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8204 - loss: 1.7088 - val_accuracy: 0.5679 - val_loss: 4.9387\n",
      "Epoch 51/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8212 - loss: 1.6883 - val_accuracy: 0.5679 - val_loss: 4.9473\n",
      "Epoch 52/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8306 - loss: 1.6357 - val_accuracy: 0.5679 - val_loss: 4.9800\n",
      "Epoch 53/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8374 - loss: 1.5841 - val_accuracy: 0.5679 - val_loss: 4.9866\n",
      "Epoch 54/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8243 - loss: 1.6080 - val_accuracy: 0.5679 - val_loss: 5.0090\n",
      "Epoch 55/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8358 - loss: 1.5528 - val_accuracy: 0.5679 - val_loss: 5.0190\n",
      "Epoch 56/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8410 - loss: 1.5107 - val_accuracy: 0.5679 - val_loss: 5.0469\n",
      "Epoch 57/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8399 - loss: 1.4900 - val_accuracy: 0.5679 - val_loss: 5.0671\n",
      "Epoch 58/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8447 - loss: 1.4601 - val_accuracy: 0.5679 - val_loss: 5.0877\n",
      "Epoch 59/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8488 - loss: 1.4231 - val_accuracy: 0.5679 - val_loss: 5.1038\n",
      "Epoch 60/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8532 - loss: 1.3969 - val_accuracy: 0.5704 - val_loss: 5.1249\n",
      "Epoch 61/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8601 - loss: 1.3666 - val_accuracy: 0.5531 - val_loss: 5.1457\n",
      "Epoch 62/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8567 - loss: 1.3520 - val_accuracy: 0.5531 - val_loss: 5.1674\n",
      "Epoch 63/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8738 - loss: 1.2835 - val_accuracy: 0.5531 - val_loss: 5.1884\n",
      "Epoch 64/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8765 - loss: 1.2603 - val_accuracy: 0.5556 - val_loss: 5.1958\n",
      "Epoch 65/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8843 - loss: 1.2323 - val_accuracy: 0.5556 - val_loss: 5.2239\n",
      "Epoch 66/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8831 - loss: 1.2104 - val_accuracy: 0.5580 - val_loss: 5.2424\n",
      "Epoch 67/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8927 - loss: 1.1853 - val_accuracy: 0.5605 - val_loss: 5.2548\n",
      "Epoch 68/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9043 - loss: 1.1254 - val_accuracy: 0.5605 - val_loss: 5.2755\n",
      "Epoch 69/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9144 - loss: 1.1033 - val_accuracy: 0.5605 - val_loss: 5.2871\n",
      "Epoch 70/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9234 - loss: 1.0516 - val_accuracy: 0.5630 - val_loss: 5.3050\n",
      "Epoch 71/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.9265 - loss: 1.0501 - val_accuracy: 0.5481 - val_loss: 5.3140\n",
      "Epoch 72/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.9306 - loss: 1.0232 - val_accuracy: 0.5481 - val_loss: 5.3404\n",
      "Epoch 73/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.9400 - loss: 0.9682 - val_accuracy: 0.5481 - val_loss: 5.3500\n",
      "Epoch 74/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.9473 - loss: 0.9503 - val_accuracy: 0.5481 - val_loss: 5.3687\n",
      "Epoch 75/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9543 - loss: 0.9224 - val_accuracy: 0.5506 - val_loss: 5.3930\n",
      "Epoch 76/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.9587 - loss: 0.9084 - val_accuracy: 0.5506 - val_loss: 5.4125\n",
      "Epoch 77/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9635 - loss: 0.8487 - val_accuracy: 0.5506 - val_loss: 5.4308\n",
      "Epoch 78/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9730 - loss: 0.8190 - val_accuracy: 0.5506 - val_loss: 5.4466\n",
      "Epoch 79/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9759 - loss: 0.7685 - val_accuracy: 0.5506 - val_loss: 5.4679\n",
      "Epoch 80/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9790 - loss: 0.7892 - val_accuracy: 0.5506 - val_loss: 5.4922\n",
      "Epoch 81/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.9815 - loss: 0.7706 - val_accuracy: 0.5506 - val_loss: 5.5098\n",
      "Epoch 82/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9846 - loss: 0.7321 - val_accuracy: 0.5506 - val_loss: 5.5349\n",
      "Epoch 83/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9867 - loss: 0.7043 - val_accuracy: 0.5506 - val_loss: 5.5428\n",
      "Epoch 84/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9873 - loss: 0.6894 - val_accuracy: 0.5506 - val_loss: 5.5674\n",
      "Epoch 85/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9878 - loss: 0.6599 - val_accuracy: 0.5506 - val_loss: 5.5807\n",
      "Epoch 86/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9869 - loss: 0.6479 - val_accuracy: 0.5506 - val_loss: 5.5974\n",
      "Epoch 87/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9882 - loss: 0.6109 - val_accuracy: 0.5506 - val_loss: 5.6131\n",
      "Epoch 88/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9896 - loss: 0.6116 - val_accuracy: 0.5506 - val_loss: 5.6226\n",
      "Epoch 89/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.9905 - loss: 0.5528 - val_accuracy: 0.5506 - val_loss: 5.6432\n",
      "Epoch 90/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.9942 - loss: 0.5456 - val_accuracy: 0.5506 - val_loss: 5.6535\n",
      "Epoch 91/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.9958 - loss: 0.5365 - val_accuracy: 0.5506 - val_loss: 5.6773\n",
      "Epoch 92/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9971 - loss: 0.5196 - val_accuracy: 0.5506 - val_loss: 5.6848\n",
      "Epoch 93/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.9950 - loss: 0.4845 - val_accuracy: 0.5506 - val_loss: 5.6960\n",
      "Epoch 94/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.9963 - loss: 0.4749 - val_accuracy: 0.5506 - val_loss: 5.7193\n",
      "Epoch 95/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9952 - loss: 0.4463 - val_accuracy: 0.5506 - val_loss: 5.7263\n",
      "Epoch 96/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9958 - loss: 0.4334 - val_accuracy: 0.5506 - val_loss: 5.7410\n",
      "Epoch 97/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.9961 - loss: 0.4259 - val_accuracy: 0.5506 - val_loss: 5.7610\n",
      "Epoch 98/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9977 - loss: 0.3927 - val_accuracy: 0.5506 - val_loss: 5.7710\n",
      "Epoch 99/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.9967 - loss: 0.3849 - val_accuracy: 0.5679 - val_loss: 5.7914\n",
      "Epoch 100/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.9967 - loss: 0.3693 - val_accuracy: 0.5679 - val_loss: 5.7950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1cd639411b0>"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4) Train\n",
    "model.fit(\n",
    "    [enc_in, dec_in],  # encoder & decoder inputs\n",
    "    dec_tar,           # one-hot “next word” targets\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    validation_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "bd1e87a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New vocab size: 170\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "print(\"New vocab size:\", VOCAB_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "9943a9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_q = max(len(x) for x in Q)\n",
    "max_a = max(len(x) for x in A)\n",
    "enc_in = pad_sequences(Q, maxlen=max_q, padding=\"post\")\n",
    "dec_in = pad_sequences(A, maxlen=max_a, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "c81504d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dec_tar = np.zeros_like(dec_in)\n",
    "for i, seq in enumerate(dec_in):\n",
    "    dec_tar[i, :-1] = seq[1:]\n",
    "VOCAB = len(tokenizer.word_index) + 1\n",
    "dec_tar = to_categorical(dec_tar, num_classes=VOCAB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8fc48a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "2ba2e140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "enc_input  = Input(shape=(None,))\n",
    "enc_embed  = Embedding(VOCAB, 32, mask_zero=True)(enc_input)\n",
    "_, h, c    = LSTM(128, return_state=True)(enc_embed)\n",
    "enc_states = [h, c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "b2643cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "enc_input  = Input(shape=(None,))\n",
    "enc_embed  = Embedding(VOCAB, 32, mask_zero=True)(enc_input)\n",
    "_, h, c    = LSTM(128, return_state=True)(enc_embed)\n",
    "enc_states = [h, c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "6aef0f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "dec_input  = Input(shape=(None,))\n",
    "dec_embed  = Embedding(VOCAB, 32, mask_zero=True)(dec_input)\n",
    "dec_lstm, _, _ = LSTM(128, return_sequences=True, return_state=True)(\n",
    "    dec_embed, initial_state=enc_states\n",
    ")\n",
    "# Apply Dense directly on the 3D output (batch, time, features):\n",
    "dec_out = Dense(\n",
    "    VOCAB_SIZE,\n",
    "    activation='softmax',\n",
    "    name='decoder_dense'\n",
    ")(dec_lstm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd216b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([enc_input, dec_input], dec_out)\n",
    "model.compile(\"rmsprop\", \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f1ac6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 139ms/step - accuracy: 0.0654 - loss: 5.1328 - val_accuracy: 0.4667 - val_loss: 5.1252\n",
      "Epoch 2/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5812 - loss: 5.1126 - val_accuracy: 0.4720 - val_loss: 5.1048\n",
      "Epoch 3/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6662 - loss: 5.0731 - val_accuracy: 0.4720 - val_loss: 5.0329\n",
      "Epoch 4/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6691 - loss: 4.9568 - val_accuracy: 0.4720 - val_loss: 4.7274\n",
      "Epoch 5/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6661 - loss: 4.6611 - val_accuracy: 0.4720 - val_loss: 4.5621\n",
      "Epoch 6/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6712 - loss: 4.3711 - val_accuracy: 0.4720 - val_loss: 4.7922\n",
      "Epoch 7/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3453 - loss: 4.1122 - val_accuracy: 0.0400 - val_loss: 4.8859\n",
      "Epoch 8/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.0512 - loss: 3.9696 - val_accuracy: 0.0800 - val_loss: 4.9008\n",
      "Epoch 9/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.3487 - loss: 3.9091 - val_accuracy: 0.4720 - val_loss: 4.9280\n",
      "Epoch 10/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6917 - loss: 3.8809 - val_accuracy: 0.4080 - val_loss: 4.9935\n",
      "Epoch 11/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.3125 - loss: 3.8216 - val_accuracy: 0.0800 - val_loss: 5.0590\n",
      "Epoch 12/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.0931 - loss: 3.7992 - val_accuracy: 0.0853 - val_loss: 5.0878\n",
      "Epoch 13/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.1008 - loss: 3.7103 - val_accuracy: 0.0853 - val_loss: 5.1404\n",
      "Epoch 14/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.0978 - loss: 3.5730 - val_accuracy: 0.0853 - val_loss: 5.1794\n",
      "Epoch 15/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1053 - loss: 3.6041 - val_accuracy: 0.0907 - val_loss: 5.1978\n",
      "Epoch 16/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1320 - loss: 3.4646 - val_accuracy: 0.0880 - val_loss: 5.2195\n",
      "Epoch 17/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1074 - loss: 3.4446 - val_accuracy: 0.0853 - val_loss: 5.2545\n",
      "Epoch 18/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.0904 - loss: 3.4094 - val_accuracy: 0.0853 - val_loss: 5.2683\n",
      "Epoch 19/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0919 - loss: 3.2965 - val_accuracy: 0.0853 - val_loss: 5.2735\n",
      "Epoch 20/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.0947 - loss: 3.2512 - val_accuracy: 0.0853 - val_loss: 5.3360\n",
      "Epoch 21/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.0877 - loss: 3.2217 - val_accuracy: 0.0853 - val_loss: 5.3329\n",
      "Epoch 22/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.0950 - loss: 3.1631 - val_accuracy: 0.0853 - val_loss: 5.3493\n",
      "Epoch 23/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0943 - loss: 3.0518 - val_accuracy: 0.0853 - val_loss: 5.4189\n",
      "Epoch 24/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0972 - loss: 3.0903 - val_accuracy: 0.0853 - val_loss: 5.3901\n",
      "Epoch 25/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.1064 - loss: 2.9996 - val_accuracy: 0.0853 - val_loss: 5.4187\n",
      "Epoch 26/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.1076 - loss: 2.9895 - val_accuracy: 0.0800 - val_loss: 5.4357\n",
      "Epoch 27/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.1099 - loss: 2.8617 - val_accuracy: 0.0800 - val_loss: 5.4556\n",
      "Epoch 28/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.1199 - loss: 2.8583 - val_accuracy: 0.0800 - val_loss: 5.4960\n",
      "Epoch 29/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1209 - loss: 2.7809 - val_accuracy: 0.0853 - val_loss: 5.5181\n",
      "Epoch 30/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.1237 - loss: 2.7932 - val_accuracy: 0.0800 - val_loss: 5.5201\n",
      "Epoch 31/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.1247 - loss: 2.6641 - val_accuracy: 0.0880 - val_loss: 5.5343\n",
      "Epoch 32/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1348 - loss: 2.6232 - val_accuracy: 0.0827 - val_loss: 5.5727\n",
      "Epoch 33/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1380 - loss: 2.5877 - val_accuracy: 0.0880 - val_loss: 5.5499\n",
      "Epoch 34/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1385 - loss: 2.5551 - val_accuracy: 0.0853 - val_loss: 5.5724\n",
      "Epoch 35/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1340 - loss: 2.5031 - val_accuracy: 0.0827 - val_loss: 5.5955\n",
      "Epoch 36/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1391 - loss: 2.3735 - val_accuracy: 0.0827 - val_loss: 5.6124\n",
      "Epoch 37/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.1320 - loss: 2.4253 - val_accuracy: 0.0827 - val_loss: 5.6286\n",
      "Epoch 38/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1360 - loss: 2.3410 - val_accuracy: 0.0827 - val_loss: 5.6481\n",
      "Epoch 39/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.1571 - loss: 2.1848 - val_accuracy: 0.0827 - val_loss: 5.6741\n",
      "Epoch 40/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.1470 - loss: 2.1648 - val_accuracy: 0.0827 - val_loss: 5.6969\n",
      "Epoch 41/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1522 - loss: 2.0925 - val_accuracy: 0.0827 - val_loss: 5.7072\n",
      "Epoch 42/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1643 - loss: 2.0143 - val_accuracy: 0.0827 - val_loss: 5.7951\n",
      "Epoch 43/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1741 - loss: 2.0273 - val_accuracy: 0.0827 - val_loss: 5.8216\n",
      "Epoch 44/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1803 - loss: 1.9666 - val_accuracy: 0.0827 - val_loss: 5.8718\n",
      "Epoch 45/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1968 - loss: 1.8256 - val_accuracy: 0.0827 - val_loss: 5.9039\n",
      "Epoch 46/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1912 - loss: 1.8920 - val_accuracy: 0.0827 - val_loss: 5.9475\n",
      "Epoch 47/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.2000 - loss: 1.7876 - val_accuracy: 0.0827 - val_loss: 5.9818\n",
      "Epoch 48/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.2130 - loss: 1.6627 - val_accuracy: 0.0827 - val_loss: 6.0407\n",
      "Epoch 49/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.2224 - loss: 1.6776 - val_accuracy: 0.0827 - val_loss: 6.0949\n",
      "Epoch 50/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.2217 - loss: 1.6460 - val_accuracy: 0.0827 - val_loss: 6.0816\n",
      "Epoch 51/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.2351 - loss: 1.5179 - val_accuracy: 0.0827 - val_loss: 6.2033\n",
      "Epoch 52/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.2279 - loss: 1.4568 - val_accuracy: 0.0827 - val_loss: 6.2320\n",
      "Epoch 53/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.2382 - loss: 1.4881 - val_accuracy: 0.0827 - val_loss: 6.2084\n",
      "Epoch 54/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.2477 - loss: 1.4026 - val_accuracy: 0.0827 - val_loss: 6.3712\n",
      "Epoch 55/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.2584 - loss: 1.2805 - val_accuracy: 0.0827 - val_loss: 6.3452\n",
      "Epoch 56/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.2742 - loss: 1.3138 - val_accuracy: 0.0827 - val_loss: 6.4223\n",
      "Epoch 57/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.2748 - loss: 1.2006 - val_accuracy: 0.0827 - val_loss: 6.5289\n",
      "Epoch 58/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.2947 - loss: 1.2018 - val_accuracy: 0.0827 - val_loss: 6.4439\n",
      "Epoch 59/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.3060 - loss: 1.1476 - val_accuracy: 0.0827 - val_loss: 6.6043\n",
      "Epoch 60/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3094 - loss: 1.0876 - val_accuracy: 0.0827 - val_loss: 6.5706\n",
      "Epoch 61/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.3177 - loss: 1.0134 - val_accuracy: 0.0827 - val_loss: 6.6275\n",
      "Epoch 62/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3392 - loss: 0.9916 - val_accuracy: 0.0827 - val_loss: 6.7253\n",
      "Epoch 63/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.3200 - loss: 0.9056 - val_accuracy: 0.0853 - val_loss: 6.6912\n",
      "Epoch 64/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.3455 - loss: 0.9397 - val_accuracy: 0.0827 - val_loss: 6.7597\n",
      "Epoch 65/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3323 - loss: 0.8882 - val_accuracy: 0.0827 - val_loss: 6.8245\n",
      "Epoch 66/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3579 - loss: 0.8545 - val_accuracy: 0.0853 - val_loss: 6.8355\n",
      "Epoch 67/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.3451 - loss: 0.7714 - val_accuracy: 0.0853 - val_loss: 6.9063\n",
      "Epoch 68/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3531 - loss: 0.7463 - val_accuracy: 0.0853 - val_loss: 6.9094\n",
      "Epoch 69/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.3440 - loss: 0.7065 - val_accuracy: 0.0880 - val_loss: 6.9767\n",
      "Epoch 70/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.3589 - loss: 0.6906 - val_accuracy: 0.0907 - val_loss: 6.9901\n",
      "Epoch 71/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.3660 - loss: 0.6630 - val_accuracy: 0.0907 - val_loss: 7.0414\n",
      "Epoch 72/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.3656 - loss: 0.6186 - val_accuracy: 0.0907 - val_loss: 7.0923\n",
      "Epoch 73/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.3490 - loss: 0.6121 - val_accuracy: 0.0907 - val_loss: 7.0988\n",
      "Epoch 74/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3528 - loss: 0.5291 - val_accuracy: 0.0907 - val_loss: 7.1624\n",
      "Epoch 75/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3510 - loss: 0.5241 - val_accuracy: 0.0907 - val_loss: 7.1851\n",
      "Epoch 76/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3634 - loss: 0.5390 - val_accuracy: 0.0907 - val_loss: 7.2555\n",
      "Epoch 77/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3442 - loss: 0.4854 - val_accuracy: 0.0907 - val_loss: 7.2494\n",
      "Epoch 78/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.3566 - loss: 0.4636 - val_accuracy: 0.0907 - val_loss: 7.3313\n",
      "Epoch 79/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.3627 - loss: 0.4485 - val_accuracy: 0.0907 - val_loss: 7.3408\n",
      "Epoch 80/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3682 - loss: 0.4149 - val_accuracy: 0.0907 - val_loss: 7.3955\n",
      "Epoch 81/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3642 - loss: 0.4023 - val_accuracy: 0.0933 - val_loss: 7.4145\n",
      "Epoch 82/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3706 - loss: 0.4045 - val_accuracy: 0.0933 - val_loss: 7.4540\n",
      "Epoch 83/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3612 - loss: 0.3742 - val_accuracy: 0.0907 - val_loss: 7.5055\n",
      "Epoch 84/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3710 - loss: 0.3567 - val_accuracy: 0.0933 - val_loss: 7.5170\n",
      "Epoch 85/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3575 - loss: 0.3148 - val_accuracy: 0.0933 - val_loss: 7.5733\n",
      "Epoch 86/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.3423 - loss: 0.3066 - val_accuracy: 0.0933 - val_loss: 7.5921\n",
      "Epoch 87/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3682 - loss: 0.2955 - val_accuracy: 0.0960 - val_loss: 7.6422\n",
      "Epoch 88/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3582 - loss: 0.2938 - val_accuracy: 0.0933 - val_loss: 7.6555\n",
      "Epoch 89/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.3573 - loss: 0.2766 - val_accuracy: 0.0960 - val_loss: 7.7183\n",
      "Epoch 90/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3542 - loss: 0.2728 - val_accuracy: 0.0960 - val_loss: 7.7317\n",
      "Epoch 91/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3446 - loss: 0.2533 - val_accuracy: 0.0960 - val_loss: 7.7752\n",
      "Epoch 92/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3897 - loss: 0.2524 - val_accuracy: 0.0960 - val_loss: 7.7681\n",
      "Epoch 93/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3772 - loss: 0.2373 - val_accuracy: 0.0960 - val_loss: 7.8553\n",
      "Epoch 94/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3477 - loss: 0.2207 - val_accuracy: 0.0960 - val_loss: 7.8541\n",
      "Epoch 95/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3646 - loss: 0.2181 - val_accuracy: 0.0960 - val_loss: 7.8832\n",
      "Epoch 96/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3800 - loss: 0.1987 - val_accuracy: 0.0960 - val_loss: 7.9252\n",
      "Epoch 97/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3665 - loss: 0.1932 - val_accuracy: 0.0960 - val_loss: 7.9576\n",
      "Epoch 98/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3786 - loss: 0.1894 - val_accuracy: 0.0987 - val_loss: 7.9740\n",
      "Epoch 99/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3658 - loss: 0.1918 - val_accuracy: 0.0987 - val_loss: 8.0042\n",
      "Epoch 100/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.3656 - loss: 0.1775 - val_accuracy: 0.0987 - val_loss: 8.0333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1cd3cdec8b0>"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7) Train\n",
    "model.fit([enc_in, dec_in], dec_tar, batch_size=64, epochs=100, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "071a2d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Clean questions\n",
    "clean_questions = [ clean_text(q) for q in training_sentences ]\n",
    "\n",
    "# 2) Clean responses\n",
    "clean_responses = [ clean_text(r) for r in responses ]\n",
    "\n",
    "# 3) Add your special tokens around each response\n",
    "target_texts    = [f\"<start> {clean_text(r)} <end>\"          for r   in clean_responses]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "5f512236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n",
      "143\n"
     ]
    }
   ],
   "source": [
    "print(len(training_sentences))      \n",
    "print(len(responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "cfe72acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sentences:  ['Hi', 'Hi there', 'Hola', 'Hello', 'Hello there', 'Hya', 'Hya there', 'My user is Adam', 'This is Adam', 'I am Adam']\n",
      "Responses:  ['Hello human, please tell me your GeniSys user', 'Hi human, please tell me your GeniSys user', 'Hola human, please tell me your GeniSys user', 'Hello human, please tell me your GeniSys user', 'Hola human, please tell me your GeniSys user', 'Hello human, please tell me your GeniSys user', 'Hi human, please tell me your GeniSys user', 'OK! Hola <HUMAN>, how can I help you?', 'Cool! Hello <HUMAN>, what can I do for you?', 'Cool! Hello <HUMAN>, what can I do for you?']\n"
     ]
    }
   ],
   "source": [
    "print(\"Training sentences: \", training_sentences[:10])  \n",
    "print(\"Responses: \", responses[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "2964e754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 tags: ['Greeting', 'GreetingResponse', 'CourtesyGreeting', 'CourtesyGreetingResponse', 'CurrentHumanQuery']\n",
      "First 5 sentences: ['Hi', 'Hi there', 'Hola', 'Hello', 'Hello there']\n",
      "First 5 responses: ['Hello human, please tell me your GeniSys user', 'Hi human, please tell me your GeniSys user', 'Hola human, please tell me your GeniSys user', 'Hello human, please tell me your GeniSys user', 'Hola human, please tell me your GeniSys user']\n",
      "Training labels: ['Greeting', 'Greeting', 'Greeting', 'Greeting', 'Greeting']\n"
     ]
    }
   ],
   "source": [
    "print('First 5 tags:', labels[:5])\n",
    "print('First 5 sentences:', training_sentences[:5])\n",
    "print('First 5 responses:', responses[:5])\n",
    "print('Training labels:', training_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "b6075aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315 unique tokens.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'start'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[484], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m word_index \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mword_index\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(word_index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m unique tokens.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m start_token_index \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstart\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      9\u001b[0m end_token_index   \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mword_index[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'start'"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = Tokenizer()  \n",
    "tokenizer.fit_on_texts(training_sentences + responses)\n",
    "question_seqs = tokenizer.texts_to_sequences(training_sentences)\n",
    "answer_seqs   = tokenizer.texts_to_sequences(responses)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(f'{len(word_index)} unique tokens.')\n",
    "start_token_index = tokenizer.word_index['start']\n",
    "end_token_index   = tokenizer.word_index['end']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "0249a3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max question length: 9\n",
      "Max answer length: 71\n"
     ]
    }
   ],
   "source": [
    "max_q = max(len(s) for s in question_seqs)\n",
    "max_a = max(len(s) for s in answer_seqs)\n",
    "\n",
    "print(f\"Max question length: {max_q}\")\n",
    "print(f\"Max answer length: {max_a}\")    \n",
    "\n",
    "padded_questions= pad_sequences(question_seqs, maxlen=max_q, padding='post')\n",
    "padded_answers = pad_sequences(answer_seqs,   maxlen=max_a, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "61aa93d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New vocab size: 316\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "print(\"New vocab size:\", VOCAB_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "a2a8c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.array(padded_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7557c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_data = np.array(padded_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "ef725bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input Shape: (143, 9)\n",
      "Decoder Input Shape: (143, 71)\n",
      "Decoder Target Shape: (143, 71, 316)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Create decoder target data by shifting padded_answers\n",
    "decoder_target_data = np.zeros_like(decoder_input_data)\n",
    "for i, seq in enumerate(decoder_input_data):\n",
    "  shifted = seq[1:] \n",
    "  decoder_target_data[i, :-1] = shifted \n",
    "  # Last element remains 0 (padding)\n",
    "\n",
    "# One-hot encode the decoder target data\n",
    "decoder_target_data = to_categorical(decoder_target_data, num_classes=VOCAB_SIZE)\n",
    "\n",
    "print(\"Encoder Input Shape:\", encoder_input_data.shape)\n",
    "print(\"Decoder Input Shape:\", decoder_input_data.shape)\n",
    "print(\"Decoder Target Shape:\", decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "ff5132b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100  \n",
    "LATENT_DIM = 128     \n",
    "BATCH_SIZE = 64     \n",
    "EPOCHS = 100         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "4f9390fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Input Layer\n",
    "# Takes sequences of integer IDs with length maxlen_questions\n",
    "encoder_inputs = Input(shape=(None,), name='encoder_inputs')\n",
    "\n",
    "# Embedding Layer\n",
    "# Converts integer sequences to dense vectors of EMBEDDING_DIM\n",
    "# mask_zero=True tells the layer to ignore padding (0s) in subsequent layers\n",
    "encoder_embedding_layer = Embedding(VOCAB_SIZE, EMBEDDING_DIM, mask_zero=True, name='encoder_embedding')\n",
    "encoder_embedding = encoder_embedding_layer(encoder_inputs)\n",
    "\n",
    "# LSTM Layer\n",
    "# Processes the embedded sequence\n",
    "# LATENT_DIM is the number of LSTM units (dimensionality of hidden/cell state)\n",
    "# return_state=True ensures the final hidden state (state_h) and cell state (state_c) are returned\n",
    "encoder_lstm = LSTM(LATENT_DIM, return_state=True, name='encoder_lstm')\n",
    "# We don't need the per-timestep outputs of the encoder, only the final states\n",
    "_, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "\n",
    "# The encoder_states contain the final hidden and cell state, capturing the input sequence context\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "f4fcb321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Input Layer\n",
    "# Takes sequences of integer IDs with length maxlen_answers (including <start>)\n",
    "decoder_inputs = Input(shape=(None,), name='decoder_inputs')\n",
    "\n",
    "# Embedding Layer (can reuse encoder's or define a new one)\n",
    "# Using a separate layer allows learning different embeddings for input vs output if needed\n",
    "decoder_embedding_layer = Embedding(VOCAB_SIZE, EMBEDDING_DIM, mask_zero=True, name='decoder_embedding')\n",
    "decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "# Decoder LSTM Layer\n",
    "# return_sequences=True is essential because we need an output at each timestep for the Dense layer\n",
    "# return_state=True is needed for inference later, although states are not directly used in this training graph connection\n",
    "decoder_lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "\n",
    "# Crucially, the decoder LSTM is initialized with the encoder_states\n",
    "# This provides the context from the input sequence\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "# Dense Output Layer\n",
    "# Projects the LSTM outputs to the vocabulary size\n",
    "# Softmax activation provides a probability distribution over the target vocabulary for each timestep\n",
    "decoder_dense = Dense(VOCAB_SIZE, activation='softmax', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "ae190b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "8cc0f62e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">31,600</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_34        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">31,600</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │ encoder_embeddin… │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),      │            │ not_equal_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │ decoder_embeddin… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_dense       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">316</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">40,764</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m) │     \u001b[38;5;34m31,600\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_34        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m) │     \u001b[38;5;34m31,600\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),     │    \u001b[38;5;34m117,248\u001b[0m │ encoder_embeddin… │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),      │            │ not_equal_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m117,248\u001b[0m │ decoder_embeddin… │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_dense       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m316\u001b[0m) │     \u001b[38;5;34m40,764\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">338,460</span> (1.29 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m338,460\u001b[0m (1.29 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">338,460</span> (1.29 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m338,460\u001b[0m (1.29 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the complete model for training\n",
    "# It takes encoder_inputs and decoder_inputs and outputs decoder_outputs\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Print a summary of the model architecture and parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "d4713921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "c41d79ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 565ms/step - accuracy: 0.0201 - loss: 5.7522 - val_accuracy: 0.7596 - val_loss: 5.7513\n",
      "Epoch 2/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.9268 - loss: 5.7362 - val_accuracy: 0.7596 - val_loss: 5.7474\n",
      "Epoch 3/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9239 - loss: 5.7219 - val_accuracy: 0.7596 - val_loss: 5.7428\n",
      "Epoch 4/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9259 - loss: 5.7047 - val_accuracy: 0.7596 - val_loss: 5.7367\n",
      "Epoch 5/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9270 - loss: 5.6857 - val_accuracy: 0.7596 - val_loss: 5.7266\n",
      "Epoch 6/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9265 - loss: 5.6528 - val_accuracy: 0.7596 - val_loss: 5.7046\n",
      "Epoch 7/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.9266 - loss: 5.5873 - val_accuracy: 0.7596 - val_loss: 5.6588\n",
      "Epoch 8/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9249 - loss: 5.4453 - val_accuracy: 0.7596 - val_loss: 5.6312\n",
      "Epoch 9/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.9232 - loss: 5.1792 - val_accuracy: 0.7596 - val_loss: 5.7349\n",
      "Epoch 10/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9268 - loss: 4.9045 - val_accuracy: 0.7596 - val_loss: 5.8675\n",
      "Epoch 11/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.9266 - loss: 4.6720 - val_accuracy: 0.7596 - val_loss: 5.9167\n",
      "Epoch 12/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.9267 - loss: 4.4556 - val_accuracy: 0.7596 - val_loss: 6.0017\n",
      "Epoch 13/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9241 - loss: 4.3529 - val_accuracy: 0.7596 - val_loss: 6.1666\n",
      "Epoch 14/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.9258 - loss: 4.2287 - val_accuracy: 0.7596 - val_loss: 6.2507\n",
      "Epoch 15/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.9283 - loss: 4.2006 - val_accuracy: 0.7596 - val_loss: 6.3455\n",
      "Epoch 16/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.9261 - loss: 4.1022 - val_accuracy: 0.7596 - val_loss: 6.4348\n",
      "Epoch 17/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.9271 - loss: 4.0523 - val_accuracy: 0.7596 - val_loss: 6.4919\n",
      "Epoch 18/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9241 - loss: 4.0509 - val_accuracy: 0.7596 - val_loss: 6.6328\n",
      "Epoch 19/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.9277 - loss: 3.9820 - val_accuracy: 0.7596 - val_loss: 6.6362\n",
      "Epoch 20/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.9228 - loss: 4.0108 - val_accuracy: 0.7596 - val_loss: 6.7855\n",
      "Epoch 21/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.9219 - loss: 3.9655 - val_accuracy: 0.7596 - val_loss: 6.8281\n",
      "Epoch 22/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.9284 - loss: 3.8971 - val_accuracy: 0.7596 - val_loss: 6.8083\n",
      "Epoch 23/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.9269 - loss: 3.9285 - val_accuracy: 0.7596 - val_loss: 6.8663\n",
      "Epoch 24/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.9239 - loss: 3.9141 - val_accuracy: 0.7596 - val_loss: 6.9599\n",
      "Epoch 25/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.9249 - loss: 3.8917 - val_accuracy: 0.7596 - val_loss: 6.9874\n",
      "Epoch 26/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9258 - loss: 3.8178 - val_accuracy: 0.7596 - val_loss: 6.9720\n",
      "Epoch 27/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.9266 - loss: 3.8529 - val_accuracy: 0.7596 - val_loss: 6.9999\n",
      "Epoch 28/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.9250 - loss: 3.8597 - val_accuracy: 0.7596 - val_loss: 7.0906\n",
      "Epoch 29/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9248 - loss: 3.8654 - val_accuracy: 0.7596 - val_loss: 7.1473\n",
      "Epoch 30/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9230 - loss: 3.8301 - val_accuracy: 0.7596 - val_loss: 7.1827\n",
      "Epoch 31/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9240 - loss: 3.7943 - val_accuracy: 0.7596 - val_loss: 7.1983\n",
      "Epoch 32/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9272 - loss: 3.7777 - val_accuracy: 0.7596 - val_loss: 7.1696\n",
      "Epoch 33/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.9232 - loss: 3.7513 - val_accuracy: 0.7596 - val_loss: 7.2328\n",
      "Epoch 34/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.9269 - loss: 3.7428 - val_accuracy: 0.7596 - val_loss: 7.2055\n",
      "Epoch 35/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9277 - loss: 3.7247 - val_accuracy: 0.7596 - val_loss: 7.2128\n",
      "Epoch 36/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.9266 - loss: 3.6825 - val_accuracy: 0.7596 - val_loss: 7.2205\n",
      "Epoch 37/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.9225 - loss: 3.6798 - val_accuracy: 0.7596 - val_loss: 7.2887\n",
      "Epoch 38/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9255 - loss: 3.6538 - val_accuracy: 0.7596 - val_loss: 7.2627\n",
      "Epoch 39/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.9251 - loss: 3.6467 - val_accuracy: 0.7596 - val_loss: 7.2872\n",
      "Epoch 40/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9236 - loss: 3.6305 - val_accuracy: 0.7596 - val_loss: 7.3370\n",
      "Epoch 41/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9259 - loss: 3.5880 - val_accuracy: 0.7596 - val_loss: 7.3040\n",
      "Epoch 42/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9184 - loss: 3.6309 - val_accuracy: 0.7596 - val_loss: 7.3110\n",
      "Epoch 43/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.9261 - loss: 3.6310 - val_accuracy: 0.7596 - val_loss: 7.3685\n",
      "Epoch 44/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9264 - loss: 3.5641 - val_accuracy: 0.7596 - val_loss: 7.3902\n",
      "Epoch 45/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9215 - loss: 3.6245 - val_accuracy: 0.7596 - val_loss: 7.4487\n",
      "Epoch 46/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.9153 - loss: 3.6295 - val_accuracy: 0.7596 - val_loss: 7.4977\n",
      "Epoch 47/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.9253 - loss: 3.5109 - val_accuracy: 0.7596 - val_loss: 7.4136\n",
      "Epoch 48/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.9175 - loss: 3.5714 - val_accuracy: 0.7596 - val_loss: 7.4349\n",
      "Epoch 49/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.9180 - loss: 3.5356 - val_accuracy: 0.7596 - val_loss: 7.4200\n",
      "Epoch 50/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.9252 - loss: 3.3965 - val_accuracy: 0.7596 - val_loss: 7.3913\n",
      "Epoch 51/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.9163 - loss: 3.5436 - val_accuracy: 0.7596 - val_loss: 7.4412\n",
      "Epoch 52/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.9161 - loss: 3.4680 - val_accuracy: 0.7596 - val_loss: 7.4285\n",
      "Epoch 53/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9158 - loss: 3.4754 - val_accuracy: 0.7596 - val_loss: 7.4841\n",
      "Epoch 54/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.9292 - loss: 3.3897 - val_accuracy: 0.7596 - val_loss: 7.3647\n",
      "Epoch 55/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.9115 - loss: 3.4818 - val_accuracy: 0.7596 - val_loss: 7.4615\n",
      "Epoch 56/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.9175 - loss: 3.4013 - val_accuracy: 0.7596 - val_loss: 7.4108\n",
      "Epoch 57/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9329 - loss: 3.3632 - val_accuracy: 0.7596 - val_loss: 7.3280\n",
      "Epoch 58/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9331 - loss: 3.2602 - val_accuracy: 0.7596 - val_loss: 7.1739\n",
      "Epoch 59/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.9217 - loss: 3.3333 - val_accuracy: 0.7596 - val_loss: 7.2901\n",
      "Epoch 60/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.9321 - loss: 3.2585 - val_accuracy: 0.7596 - val_loss: 7.1412\n",
      "Epoch 61/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.9312 - loss: 3.3026 - val_accuracy: 0.7596 - val_loss: 7.1857\n",
      "Epoch 62/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.9359 - loss: 3.2313 - val_accuracy: 0.7596 - val_loss: 7.1342\n",
      "Epoch 63/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.9340 - loss: 3.2001 - val_accuracy: 0.7596 - val_loss: 7.0936\n",
      "Epoch 64/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9311 - loss: 3.2033 - val_accuracy: 0.7596 - val_loss: 7.0480\n",
      "Epoch 65/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.9311 - loss: 3.1928 - val_accuracy: 0.7596 - val_loss: 7.0786\n",
      "Epoch 66/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9364 - loss: 3.1903 - val_accuracy: 0.7596 - val_loss: 7.0457\n",
      "Epoch 67/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.9361 - loss: 3.1168 - val_accuracy: 0.7596 - val_loss: 6.8461\n",
      "Epoch 68/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.9353 - loss: 3.1198 - val_accuracy: 0.7596 - val_loss: 6.8332\n",
      "Epoch 69/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9375 - loss: 3.0405 - val_accuracy: 0.7596 - val_loss: 6.8014\n",
      "Epoch 70/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.9356 - loss: 3.0509 - val_accuracy: 0.7596 - val_loss: 6.8268\n",
      "Epoch 71/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9388 - loss: 2.9584 - val_accuracy: 0.7596 - val_loss: 6.8264\n",
      "Epoch 72/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9368 - loss: 2.9961 - val_accuracy: 0.7596 - val_loss: 6.7850\n",
      "Epoch 73/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.9413 - loss: 2.8840 - val_accuracy: 0.7596 - val_loss: 6.7196\n",
      "Epoch 74/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.9339 - loss: 2.9150 - val_accuracy: 0.7596 - val_loss: 6.6774\n",
      "Epoch 75/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9356 - loss: 2.9407 - val_accuracy: 0.7596 - val_loss: 6.7298\n",
      "Epoch 76/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9406 - loss: 2.8325 - val_accuracy: 0.7596 - val_loss: 6.7215\n",
      "Epoch 77/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9411 - loss: 2.7747 - val_accuracy: 0.7596 - val_loss: 6.9365\n",
      "Epoch 78/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9412 - loss: 2.7899 - val_accuracy: 0.7596 - val_loss: 6.8458\n",
      "Epoch 79/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9400 - loss: 2.7541 - val_accuracy: 0.7596 - val_loss: 6.8236\n",
      "Epoch 80/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9417 - loss: 2.7270 - val_accuracy: 0.7596 - val_loss: 6.8835\n",
      "Epoch 81/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.9413 - loss: 2.7466 - val_accuracy: 0.7601 - val_loss: 6.8474\n",
      "Epoch 82/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9361 - loss: 2.7233 - val_accuracy: 0.7596 - val_loss: 6.8007\n",
      "Epoch 83/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9375 - loss: 2.6361 - val_accuracy: 0.7596 - val_loss: 6.7043\n",
      "Epoch 84/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.9449 - loss: 2.5666 - val_accuracy: 0.7596 - val_loss: 6.8202\n",
      "Epoch 85/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9449 - loss: 2.6339 - val_accuracy: 0.7596 - val_loss: 6.8599\n",
      "Epoch 86/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9433 - loss: 2.5446 - val_accuracy: 0.7581 - val_loss: 6.8294\n",
      "Epoch 87/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9464 - loss: 2.4956 - val_accuracy: 0.7596 - val_loss: 6.8668\n",
      "Epoch 88/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9482 - loss: 2.4409 - val_accuracy: 0.7441 - val_loss: 6.8435\n",
      "Epoch 89/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.9443 - loss: 2.5205 - val_accuracy: 0.7586 - val_loss: 6.9232\n",
      "Epoch 90/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9491 - loss: 2.4115 - val_accuracy: 0.7402 - val_loss: 6.8833\n",
      "Epoch 91/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.9479 - loss: 2.4106 - val_accuracy: 0.7484 - val_loss: 6.9633\n",
      "Epoch 92/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.9489 - loss: 2.3550 - val_accuracy: 0.7576 - val_loss: 7.0057\n",
      "Epoch 93/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9512 - loss: 2.3427 - val_accuracy: 0.7465 - val_loss: 7.0105\n",
      "Epoch 94/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9476 - loss: 2.3541 - val_accuracy: 0.7572 - val_loss: 7.1386\n",
      "Epoch 95/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.9520 - loss: 2.2553 - val_accuracy: 0.7567 - val_loss: 7.1101\n",
      "Epoch 96/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9553 - loss: 2.2009 - val_accuracy: 0.7523 - val_loss: 7.1093\n",
      "Epoch 97/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9537 - loss: 2.2228 - val_accuracy: 0.7596 - val_loss: 7.1692\n",
      "Epoch 98/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9501 - loss: 2.2401 - val_accuracy: 0.7572 - val_loss: 7.1913\n",
      "Epoch 99/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9543 - loss: 2.1917 - val_accuracy: 0.7596 - val_loss: 7.2056\n",
      "Epoch 100/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9543 - loss: 2.1447 - val_accuracy: 0.7572 - val_loss: 7.2003\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "history = model.fit([encoder_input_data, decoder_input_data], # Input data (list)\n",
    "                    decoder_target_data,                     # Target data\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_split=0.2)                \n",
    "# Use 20% of data for validation\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "23df7001",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('chatbot_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "3cf92bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "# 1) Grab your original embedding + LSTM layers by name:\n",
    "emb_layer  = model.get_layer('encoder_embedding')  # the Embedding you used on encoder side\n",
    "lstm_layer = model.get_layer('encoder_lstm')       # the LSTM you used with return_state=True\n",
    "\n",
    "# 2) Re-define a fresh Input for inference:\n",
    "enc_inputs = Input(shape=(None,), name='enc_input_inf')\n",
    "\n",
    "# 3) Re-apply the same layers to that input:\n",
    "enc_emb    = emb_layer(enc_inputs)\n",
    "# Since you originally did something like\n",
    "#   _, state_h, state_c = LSTM(..., return_state=True)(enc_emb)\n",
    "# you can do the same here:\n",
    "_, state_h, state_c = lstm_layer(enc_emb)\n",
    "\n",
    "# 4) Build the encoder‐only model that outputs the states:\n",
    "encoder_model = Model(enc_inputs, [state_h, state_c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "409525f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "# 1) Grab the trained decoder‐side layers by name:\n",
    "dec_emb_layer  = model.get_layer(\"decoder_embedding\")\n",
    "dec_lstm_layer = model.get_layer(\"decoder_lstm\")\n",
    "dec_dense      = model.get_layer(\"decoder_dense\")   # ← here\n",
    "\n",
    "# 2) Build the single‐step decoder for inference:\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "decoder_input_token = Input(shape=(1,),   name=\"dec_input_token\")\n",
    "state_input_h       = Input(shape=(LATENT_DIM,), name=\"dec_state_h\")\n",
    "state_input_c       = Input(shape=(LATENT_DIM,), name=\"dec_state_c\")\n",
    "\n",
    "dec_emb_inf = dec_emb_layer(decoder_input_token)\n",
    "dec_outputs, h_new, c_new = dec_lstm_layer(\n",
    "    dec_emb_inf,\n",
    "    initial_state=[state_input_h, state_input_c]\n",
    ")\n",
    "\n",
    "# apply your Dense (which will run over the time axis automatically)\n",
    "token_probs = dec_dense(dec_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_input_token, state_input_h, state_input_c],\n",
    "    [token_probs, h_new, c_new]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "0946d097",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'start'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[483], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m start_token_index \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstart\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      3\u001b[0m end_token_index   \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mword_index[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Reverse map from integer → word (so you can turn IDs back into strings)\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'start'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "start_token_index = tokenizer.word_index['start']\n",
    "end_token_index   = tokenizer.word_index['end']\n",
    "\n",
    "# Reverse map from integer → word (so you can turn IDs back into strings)\n",
    "def decode_sequence(input_text):\n",
    "    # 1) Encode the input question\n",
    "    seq    = tokenizer.texts_to_sequences([clean_text(input_text)])\n",
    "    padded = pad_sequences(seq, maxlen=max_q, padding=\"post\")\n",
    "    h, c   = encoder_model.predict(padded)\n",
    "\n",
    "    # 2) Seed the decoder with the bare “start” token ID\n",
    "    current_token = np.array([[ start_token_index ]])\n",
    "    decoded = []\n",
    "\n",
    "    # 3) Step by step generation\n",
    "    for _ in range(max_a):\n",
    "        probs, h, c = decoder_model.predict([current_token, h, c])\n",
    "        next_id     = probs[0, -1].argmax()\n",
    "\n",
    "        # If we hit the “end” token, stop early\n",
    "        if next_id == end_token_index:\n",
    "            break\n",
    "\n",
    "        # Otherwise turn that ID back into a word\n",
    "        decoded.append(reverse_word_index[next_id])\n",
    "\n",
    "        # And feed it back into the next step\n",
    "        current_token = np.array([[ next_id ]])\n",
    "\n",
    "    return \" \".join(decoded)\n",
    "\n",
    "# Now this will work without KeyErrors:\n",
    "print(decode_sequence(\"Hello there!\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037bf09d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer \"functional_5\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(1, 9) dtype=int32>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[447], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m random\u001b[38;5;241m.\u001b[39mchoice(intent2resps[intent])\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# example\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mbot_response\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[447], line 17\u001b[0m, in \u001b[0;36mbot_response\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbot_response\u001b[39m(text):\n\u001b[1;32m---> 17\u001b[0m     intent, conf \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_intent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m conf \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.3\u001b[39m:\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSorry, I didn’t get that. Can you rephrase?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[447], line 10\u001b[0m, in \u001b[0;36mpredict_intent\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      8\u001b[0m seq \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences([text])\n\u001b[0;32m      9\u001b[0m p   \u001b[38;5;241m=\u001b[39m pad_sequences(seq, maxlen\u001b[38;5;241m=\u001b[39mmaxlen, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m pred\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     11\u001b[0m idx \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39margmax()\n\u001b[0;32m     12\u001b[0m intent_name \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39minverse_transform([idx])[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\murta\\Desktop\\ML&DE\\Fourth_Semester\\Advanced_ML\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\murta\\Desktop\\ML&DE\\Fourth_Semester\\Advanced_ML\\.venv\\lib\\site-packages\\keras\\src\\layers\\input_spec.py:160\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    158\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mflatten(inputs)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_spec):\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expects \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(input_spec)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input(s),\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but it received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input tensors. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputs received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_index, (x, spec) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(inputs, input_spec)):\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Layer \"functional_5\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(1, 9) dtype=int32>]"
     ]
    }
   ],
   "source": [
    "import random\n",
    "intent2resps = {\n",
    "  intent[\"intent\"]: intent[\"responses\"]\n",
    "  for intent in data[\"intents\"]\n",
    "}\n",
    "\n",
    "def predict_intent(text):\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    p   = pad_sequences(seq, maxlen=maxlen, padding='post')\n",
    "    pred= model.predict(p)[0]\n",
    "    idx = pred.argmax()\n",
    "    intent_name = le.inverse_transform([idx])[0]\n",
    "    conf = pred[idx]\n",
    "    return intent_name, conf    \n",
    "\n",
    "def bot_response(text):\n",
    "    intent, conf = predict_intent(text)\n",
    "    if conf < 0.3:\n",
    "        return \"Sorry, I didn’t get that. Can you rephrase?\"\n",
    "    return random.choice(intent2resps[intent])\n",
    "\n",
    "# example\n",
    "print(bot_response(\"Hi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f5b812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Tokenize using the loaded tokenizer\n",
    "sequence = tokenizer.texts_to_sequences([text]) # Input must be a list\n",
    "\n",
    "# 3. Pad the sequence to maxlen_questions\n",
    "padded_sequence = pad_sequences(sequence, maxlen=maxlen_questions, padding='post')\n",
    "\n",
    "return padded_sequence # Shape: (1, maxlen_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058c6598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It is short list of each intent name only once.\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ce8c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One label for every training sentence\n",
    "len(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9378015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 143\n",
      "Number of labels: 143\n"
     ]
    }
   ],
   "source": [
    "# show the number of sentences and labels\n",
    "print('Number of sentences:', len(training_sentences))\n",
    "print('Number of labels:', len(training_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020b768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a tokenizer\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(training_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a2bf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each sentence into a list of integers.\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "# find the length of the longest sentence\n",
    "maxlen    = max(len(seq) for seq in sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dab6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 118, maxlen: 9\n"
     ]
    }
   ],
   "source": [
    "X = pad_sequences(sequences, maxlen=maxlen, padding='post')\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f'Vocab size: {vocab_size}, maxlen: {maxlen}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffecba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "   # Save the tokenizer\n",
    "with open('tokenizer.pkl', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b5d6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceaeb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input Shape: (143, 9)\n"
     ]
    }
   ],
   "source": [
    "print('Encoder Input Shape:', encoder_input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6854b853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn my intent names into integer codes.\n",
    "le    = LabelEncoder()\n",
    "y_int = le.fit_transform(training_labels)      \n",
    "y     = to_categorical(y_int)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7a18e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 22\n"
     ]
    }
   ],
   "source": [
    "num_classes = y.shape[1]\n",
    "print('Number of classes:', num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15ee667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 128\n",
      "Validation size: 15\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42\n",
    ")\n",
    "print('Training size:', len(X_train))\n",
    "print('Validation size:', len(X_val))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dd4b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 32\n",
    "LATENT_DIM = 128     \n",
    "BATCH_SIZE = 64    \n",
    "EPOCHS = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d18a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "# Encoder Input Layer\n",
    "# Takes sequences of integer IDs with length maxlen_questions\n",
    "encoder_inputs = Input(shape=(maxlen,), name='encoder_inputs')\n",
    "\n",
    "# Embedding Layer\n",
    "# Converts integer sequences to dense vectors of EMBEDDING_DIM\n",
    "# mask_zero=True tells the layer to ignore padding (0s) in subsequent layers\n",
    "encoder_embedding_layer = Embedding(vocab_size, EMBEDDING_DIM, mask_zero=True, name='encoder_embedding')\n",
    "encoder_embedding = encoder_embedding_layer(encoder_inputs)\n",
    "\n",
    "# LSTM Layer\n",
    "# Processes the embedded sequence\n",
    "# LATENT_DIM is the number of LSTM units (dimensionality of hidden/cell state)\n",
    "# return_state=True ensures the final hidden state (state_h) and cell state (state_c) are returned\n",
    "encoder_lstm = LSTM(LATENT_DIM, return_state=True, name='encoder_lstm')\n",
    "# We don't need the per-timestep outputs of the encoder, only the final states\n",
    "_, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "\n",
    "# The encoder_states contain the final hidden and cell state, capturing the input sequence context\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d110fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Input Layer\n",
    "# Takes sequences of integer IDs with length maxlen_answers (including <start>)\n",
    "decoder_inputs = Input(shape=(maxlen_answers,), name='decoder_inputs')\n",
    "\n",
    "# Embedding Layer (can reuse encoder's or define a new one)\n",
    "# Using a separate layer allows learning different embeddings for input vs output if needed\n",
    "decoder_embedding_layer = Embedding(VOCAB_SIZE, EMBEDDING_DIM, mask_zero=True, name='decoder_embedding')\n",
    "decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "# Decoder LSTM Layer\n",
    "# return_sequences=True is essential because we need an output at each timestep for the Dense layer\n",
    "# return_state=True is needed for inference later, although states are not directly used in this training graph connection\n",
    "decoder_lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "\n",
    "# Crucially, the decoder LSTM is initialized with the encoder_states\n",
    "# This provides the context from the input sequence\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "# Dense Output Layer\n",
    "# Projects the LSTM outputs to the vocabulary size\n",
    "# Softmax activation provides a probability distribution over the target vocabulary for each timestep\n",
    "decoder_dense = Dense(VOCAB_SIZE, activation='softmax', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba72911b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (128,)\n"
     ]
    }
   ],
   "source": [
    "# np.argmax(..., axis=1) finds the index of the 1 in each row\n",
    "# turning each one hot vector back into a single integer label\n",
    "# Since I am going to use sparse_categorical_crossentropy as the loss function\n",
    "# it expects integer class IDs, not one hot\n",
    "\"\"\"y_train = np.argmax(y_train,   axis=1)\n",
    "y_val   = np.argmax(y_val,     axis=1)  \n",
    "print('Training set shape:', y_train.shape)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dc19b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\murta\\Desktop\\ML&DE\\Fourth_Semester\\Advanced_ML\\.venv\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embed_dim = 32\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=maxlen, mask_zero=True),\n",
    "    LSTM(64),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755f08f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - accuracy: 0.0594 - loss: 3.0907 - val_accuracy: 0.0667 - val_loss: 3.0924\n",
      "Epoch 2/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.1250 - loss: 3.0853 - val_accuracy: 0.0000e+00 - val_loss: 3.0913\n",
      "Epoch 3/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.2917 - loss: 3.0805 - val_accuracy: 0.0000e+00 - val_loss: 3.0903\n",
      "Epoch 4/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.2750 - loss: 3.0749 - val_accuracy: 0.0000e+00 - val_loss: 3.0888\n",
      "Epoch 5/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3552 - loss: 3.0657 - val_accuracy: 0.0000e+00 - val_loss: 3.0873\n",
      "Epoch 6/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.3729 - loss: 3.0564 - val_accuracy: 0.0000e+00 - val_loss: 3.0859\n",
      "Epoch 7/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3292 - loss: 3.0496 - val_accuracy: 0.0000e+00 - val_loss: 3.0834\n",
      "Epoch 8/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.3719 - loss: 3.0388 - val_accuracy: 0.0000e+00 - val_loss: 3.0812\n",
      "Epoch 9/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3125 - loss: 3.0304 - val_accuracy: 0.0000e+00 - val_loss: 3.0781\n",
      "Epoch 10/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.3417 - loss: 3.0116 - val_accuracy: 0.0000e+00 - val_loss: 3.0750\n",
      "Epoch 11/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3104 - loss: 2.9870 - val_accuracy: 0.0000e+00 - val_loss: 3.0723\n",
      "Epoch 12/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.3500 - loss: 2.9405 - val_accuracy: 0.0000e+00 - val_loss: 3.0716\n",
      "Epoch 13/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3104 - loss: 2.8996 - val_accuracy: 0.0000e+00 - val_loss: 3.0745\n",
      "Epoch 14/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.3198 - loss: 2.8326 - val_accuracy: 0.0000e+00 - val_loss: 3.0938\n",
      "Epoch 15/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.2438 - loss: 2.7267 - val_accuracy: 0.0000e+00 - val_loss: 3.1503\n",
      "Epoch 16/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.2573 - loss: 2.6613 - val_accuracy: 0.0000e+00 - val_loss: 3.2365\n",
      "Epoch 17/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.3031 - loss: 2.5549 - val_accuracy: 0.0000e+00 - val_loss: 3.3246\n",
      "Epoch 18/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3292 - loss: 2.4315 - val_accuracy: 0.0000e+00 - val_loss: 3.3475\n",
      "Epoch 19/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.3344 - loss: 2.3708 - val_accuracy: 0.0000e+00 - val_loss: 3.3749\n",
      "Epoch 20/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.3219 - loss: 2.3750 - val_accuracy: 0.0000e+00 - val_loss: 3.4058\n",
      "Epoch 21/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.4010 - loss: 2.2127 - val_accuracy: 0.0667 - val_loss: 3.4330\n",
      "Epoch 22/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.3812 - loss: 2.2386 - val_accuracy: 0.0667 - val_loss: 3.4579\n",
      "Epoch 23/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.4500 - loss: 2.1356 - val_accuracy: 0.0667 - val_loss: 3.4735\n",
      "Epoch 24/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4729 - loss: 2.0861 - val_accuracy: 0.0667 - val_loss: 3.4890\n",
      "Epoch 25/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.4719 - loss: 1.9481 - val_accuracy: 0.0667 - val_loss: 3.5101\n",
      "Epoch 26/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.4990 - loss: 1.9918 - val_accuracy: 0.0667 - val_loss: 3.5272\n",
      "Epoch 27/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.4844 - loss: 1.9587 - val_accuracy: 0.0667 - val_loss: 3.5349\n",
      "Epoch 28/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5115 - loss: 1.8472 - val_accuracy: 0.0667 - val_loss: 3.5320\n",
      "Epoch 29/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5406 - loss: 1.8281 - val_accuracy: 0.0667 - val_loss: 3.5369\n",
      "Epoch 30/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5781 - loss: 1.7511 - val_accuracy: 0.0667 - val_loss: 3.4972\n",
      "Epoch 31/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5594 - loss: 1.7240 - val_accuracy: 0.0667 - val_loss: 3.4747\n",
      "Epoch 32/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6406 - loss: 1.6140 - val_accuracy: 0.1333 - val_loss: 3.4570\n",
      "Epoch 33/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5917 - loss: 1.6413 - val_accuracy: 0.1333 - val_loss: 3.4575\n",
      "Epoch 34/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6844 - loss: 1.5391 - val_accuracy: 0.1333 - val_loss: 3.4436\n",
      "Epoch 35/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7042 - loss: 1.4766 - val_accuracy: 0.2000 - val_loss: 3.3796\n",
      "Epoch 36/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6781 - loss: 1.4961 - val_accuracy: 0.2000 - val_loss: 3.3537\n",
      "Epoch 37/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.6844 - loss: 1.4682 - val_accuracy: 0.4000 - val_loss: 3.3415\n",
      "Epoch 38/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7437 - loss: 1.3934 - val_accuracy: 0.4667 - val_loss: 3.2997\n",
      "Epoch 39/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7875 - loss: 1.3005 - val_accuracy: 0.5333 - val_loss: 3.2264\n",
      "Epoch 40/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7875 - loss: 1.3521 - val_accuracy: 0.5333 - val_loss: 3.1947\n",
      "Epoch 41/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8260 - loss: 1.2221 - val_accuracy: 0.5333 - val_loss: 3.1585\n",
      "Epoch 42/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7844 - loss: 1.2553 - val_accuracy: 0.5333 - val_loss: 3.1114\n",
      "Epoch 43/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7708 - loss: 1.3671 - val_accuracy: 0.5333 - val_loss: 3.0552\n",
      "Epoch 44/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8229 - loss: 1.2216 - val_accuracy: 0.5333 - val_loss: 3.0257\n",
      "Epoch 45/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7917 - loss: 1.1712 - val_accuracy: 0.5333 - val_loss: 3.0081\n",
      "Epoch 46/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8104 - loss: 1.1071 - val_accuracy: 0.5333 - val_loss: 2.9870\n",
      "Epoch 47/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8813 - loss: 1.0669 - val_accuracy: 0.5333 - val_loss: 2.9669\n",
      "Epoch 48/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7833 - loss: 1.0922 - val_accuracy: 0.6000 - val_loss: 2.9321\n",
      "Epoch 49/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8354 - loss: 1.0094 - val_accuracy: 0.6000 - val_loss: 2.8707\n",
      "Epoch 50/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8635 - loss: 1.0322 - val_accuracy: 0.6000 - val_loss: 2.8472\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fd4a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Sorry, I didn’t get that. Can you rephrase?\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "intent2resps = {\n",
    "  intent[\"intent\"]: intent[\"responses\"]\n",
    "  for intent in data[\"intents\"]\n",
    "}\n",
    "\n",
    "def predict_intent(text):\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    p   = pad_sequences(seq, maxlen=maxlen, padding='post')\n",
    "    pred= model.predict(p)[0]\n",
    "    idx = pred.argmax()\n",
    "    intent_name = le.inverse_transform([idx])[0]\n",
    "    conf = pred[idx]\n",
    "    return intent_name, conf    \n",
    "\n",
    "def bot_response(text):\n",
    "    intent, conf = predict_intent(text)\n",
    "    if conf < 0.3:\n",
    "        return \"Sorry, I didn’t get that. Can you rephrase?\"\n",
    "    return random.choice(intent2resps[intent])\n",
    "\n",
    "# example\n",
    "print(bot_response(\"Hi\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
